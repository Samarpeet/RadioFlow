{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü©ª RadioFlow: AI-Powered Radiology Workflow Agent\n",
        "## MedGemma Impact Challenge Submission\n",
        "\n",
        "**Author:** Samarpeet Garad | **Date:** February 2026\n",
        "\n",
        "**GitHub:** https://github.com/Samarpeet/RadioFlow\n",
        "\n",
        "---\n",
        "\n",
        "### Features:\n",
        "- ü§ñ Real MedGemma-4B inference\n",
        "- üî¨ 4-agent pipeline\n",
        "- üìã Structured radiology reports\n",
        "- üö¶ Priority assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional, Any\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from IPython.display import HTML, display\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "GPU_AVAILABLE = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authenticate with Hugging Face\n",
        "\n",
        "Add your HF_TOKEN as a Kaggle secret. Accept the license at https://huggingface.co/google/medgemma-4b-it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    HF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úÖ Authenticated with HuggingFace\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Add HF_TOKEN as Kaggle secret: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load MedGemma Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"google/medgemma-4b-it\"\n",
        "MODEL_LOADED = False\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "print(f\"üîÑ Loading {MODEL_NAME}...\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model.eval()\n",
        "    MODEL_LOADED = True\n",
        "    print(\"‚úÖ MedGemma loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed: {e}\")\n",
        "    MODEL_LOADED = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(prompt, max_tokens=256):\n",
        "    \"\"\"Generate response using MedGemma.\"\"\"\n",
        "    if not MODEL_LOADED:\n",
        "        return \"[Demo mode]\"\n",
        "    try:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
        "        inputs = inputs.to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(inputs, max_new_tokens=max_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "    except Exception as e:\n",
        "        return f\"[Error: {e}]\"\n",
        "\n",
        "# Test\n",
        "if MODEL_LOADED:\n",
        "    print(\"üß™ Testing MedGemma...\")\n",
        "    print(generate_response(\"List 3 chest X-ray findings briefly.\", 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AgentResult:\n",
        "    agent_name: str\n",
        "    status: str\n",
        "    data: Dict[str, Any]\n",
        "    processing_time_ms: float\n",
        "\n",
        "class CXRAnalyzer:\n",
        "    def __call__(self, image, context=None):\n",
        "        start = time.time()\n",
        "        arr = np.array(image.convert(\"L\"))\n",
        "        h, w = arr.shape\n",
        "        stats = {\"right\": arr[:, w//2:].mean(), \"left\": arr[:, :w//2].mean(), \n",
        "                 \"lower\": arr[h//2:, :].mean(), \"overall\": arr.mean()}\n",
        "        findings = []\n",
        "        if stats[\"lower\"] > stats[\"overall\"] + 10:\n",
        "            findings.append({\"type\": \"opacity\", \"region\": \"lower_lungs\", \"severity\": \"moderate\", \"description\": \"Increased lower lung density\"})\n",
        "        if abs(stats[\"right\"] - stats[\"left\"]) > 12:\n",
        "            findings.append({\"type\": \"asymmetry\", \"region\": \"hemithorax\", \"severity\": \"mild\", \"description\": \"Asymmetric lung density\"})\n",
        "        if not findings:\n",
        "            findings.append({\"type\": \"normal\", \"region\": \"lungs\", \"severity\": \"none\", \"description\": \"No abnormalities\"})\n",
        "        return AgentResult(\"CXR Analyzer\", \"success\", {\"findings\": findings}, (time.time()-start)*1000)\n",
        "\n",
        "class FindingInterpreter:\n",
        "    def __call__(self, data, context=None):\n",
        "        start = time.time()\n",
        "        interpreted = []\n",
        "        for f in data.get(\"findings\", []):\n",
        "            if MODEL_LOADED and f[\"type\"] != \"normal\":\n",
        "                interp = generate_response(f\"Interpret this X-ray finding: {f['description']}\", 80)\n",
        "            else:\n",
        "                interp = f[\"description\"]\n",
        "            interpreted.append({\"original\": f, \"interpretation\": interp})\n",
        "        return AgentResult(\"Interpreter\", \"success\", {\"interpreted\": interpreted}, (time.time()-start)*1000)\n",
        "\n",
        "class ReportGenerator:\n",
        "    def __call__(self, data, context=None):\n",
        "        start = time.time()\n",
        "        findings_text = \"\\n\".join([f\"- {i['original']['description']}\" for i in data.get(\"interpreted\", [])])\n",
        "        history = context.get(\"clinical_history\", \"Not provided\") if context else \"Not provided\"\n",
        "        if MODEL_LOADED:\n",
        "            prompt = f\"Generate a brief chest X-ray report.\\nHistory: {history}\\nFindings:\\n{findings_text}\\n\\nInclude: TECHNIQUE, FINDINGS, IMPRESSION\"\n",
        "            report = generate_response(prompt, 300)\n",
        "        else:\n",
        "            report = f\"TECHNIQUE: PA chest X-ray\\n\\nFINDINGS:\\n{findings_text}\\n\\nIMPRESSION: Clinical correlation recommended.\"\n",
        "        full = f\"\\n{'='*60}\\nCHEST RADIOGRAPH REPORT\\n{'='*60}\\n\\n{report}\\n\\n{'='*60}\\nGenerated by RadioFlow\\n{'='*60}\"\n",
        "        return AgentResult(\"Reporter\", \"success\", {\"report\": full}, (time.time()-start)*1000)\n",
        "\n",
        "class PriorityRouter:\n",
        "    def __call__(self, data, context=None):\n",
        "        start = time.time()\n",
        "        findings = context.get(\"findings\", []) if context else []\n",
        "        score = 0.2\n",
        "        for f in findings:\n",
        "            if f.get(\"severity\") == \"moderate\": score = max(score, 0.5)\n",
        "            if f.get(\"severity\") == \"high\": score = max(score, 0.8)\n",
        "        level = \"STAT\" if score >= 0.8 else \"URGENT\" if score >= 0.5 else \"ROUTINE\"\n",
        "        return AgentResult(\"Router\", \"success\", {\"level\": level, \"score\": score}, (time.time()-start)*1000)\n",
        "\n",
        "print(\"‚úÖ All agents defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RadioFlowOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.analyzer = CXRAnalyzer()\n",
        "        self.interpreter = FindingInterpreter()\n",
        "        self.reporter = ReportGenerator()\n",
        "        self.router = PriorityRouter()\n",
        "    \n",
        "    def process(self, image, context=None):\n",
        "        context = context or {}\n",
        "        print(\"\\nüî¨ Stage 1: Analyzing...\")\n",
        "        r1 = self.analyzer(image, context)\n",
        "        print(f\"   Found {len(r1.data['findings'])} findings\")\n",
        "        \n",
        "        print(\"üìã Stage 2: Interpreting...\")\n",
        "        r2 = self.interpreter(r1.data, context)\n",
        "        \n",
        "        print(\"üìù Stage 3: Generating report...\")\n",
        "        r3 = self.reporter(r2.data, context)\n",
        "        \n",
        "        print(\"üö¶ Stage 4: Priority...\")\n",
        "        ctx = {**context, \"findings\": r1.data[\"findings\"]}\n",
        "        r4 = self.router(r3.data, ctx)\n",
        "        print(f\"   Priority: {r4.data['level']}\")\n",
        "        \n",
        "        return {\"report\": r3.data[\"report\"], \"priority\": r4.data[\"level\"], \n",
        "                \"score\": r4.data[\"score\"], \"findings_count\": len(r1.data[\"findings\"]),\n",
        "                \"agents\": [r1, r2, r3, r4]}\n",
        "\n",
        "orchestrator = RadioFlowOrchestrator()\n",
        "print(\"üöÄ RadioFlow ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sample_xray(size=(512, 512)):\n",
        "    np.random.seed(42)\n",
        "    img = Image.new(\"L\", size, 30)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    w, h = size\n",
        "    draw.ellipse([50, 80, w//2-20, h-50], fill=20)\n",
        "    draw.ellipse([w//2+20, 80, w-50, h-50], fill=28)\n",
        "    draw.ellipse([w//3, h//3, 2*w//3, 2*h//3], fill=75)\n",
        "    arr = np.array(img)\n",
        "    arr = np.clip(arr + np.random.normal(0, 4, arr.shape), 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(arr).convert(\"RGB\")\n",
        "\n",
        "sample_image = create_sample_xray()\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(sample_image, cmap=\"gray\")\n",
        "plt.title(\"Sample Chest X-Ray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run RadioFlow\n",
        "result = orchestrator.process(sample_image, {\"clinical_history\": \"65yo with cough and fever\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Report\n",
        "print(result[\"report\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Priority Display\n",
        "colors = {\"STAT\": \"#ef4444\", \"URGENT\": \"#f59e0b\", \"ROUTINE\": \"#22c55e\"}\n",
        "display(HTML(f\"\"\"\n",
        "<div style=\"padding:20px; background:linear-gradient(135deg,#1e3a5f,#2d5a87); border-radius:12px; color:white;\">\n",
        "<h2>üö¶ Priority: <span style=\"color:{colors[result['priority']]}\">{result['priority']}</span></h2>\n",
        "<p>Score: {result['score']:.0%} | Findings: {result['findings_count']}</p>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent Metrics\n",
        "metrics = pd.DataFrame([{\"Agent\": a.agent_name, \"Time (ms)\": f\"{a.processing_time_ms:.0f}\"} for a in result[\"agents\"]])\n",
        "display(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "‚úÖ **RadioFlow** - 4-agent radiology AI using MedGemma\n",
        "\n",
        "| Feature | Implementation |\n",
        "|---------|----------------|\n",
        "| MedGemma | Real inference |\n",
        "| Agents | 4-stage pipeline |\n",
        "| Reports | Structured format |\n",
        "\n",
        "**GitHub:** https://github.com/Samarpeet/RadioFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüèÜ RadioFlow Complete!\")\n",
        "print(f\"Model: {'MedGemma (REAL)' if MODEL_LOADED else 'Demo'}\")\n",
        "print(f\"Priority: {result['priority']}\")\n",
        "print(\"GitHub: https://github.com/Samarpeet/RadioFlow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Agents\n",
        "\n",
        "RadioFlow uses 4 agents:\n",
        "1. **CXR Analyzer** - Analyzes X-ray images\n",
        "2. **Finding Interpreter** - Uses MedGemma to interpret findings\n",
        "3. **Report Generator** - Creates structured reports\n",
        "4. **Priority Router** - Assesses urgency"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
